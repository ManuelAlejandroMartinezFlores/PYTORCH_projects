{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19 RNN & LSTM & GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28\n",
    "sequence_length = 28\n",
    "n_layers = 2\n",
    "hidden_size = 128\n",
    "num_classes = 10 \n",
    "nepochs = 2\n",
    "batch_size = 100\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True,\n",
    "                                           transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False,\n",
    "                                           transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "examples = iter(train_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(samples[i][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size, hidden_size, n_layers, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 2, step  100/600, loss 0.97657\n",
      "epoch 1 / 2, step  200/600, loss 0.57002\n",
      "epoch 1 / 2, step  300/600, loss 0.58815\n",
      "epoch 1 / 2, step  400/600, loss 0.36561\n",
      "epoch 1 / 2, step  500/600, loss 0.32704\n",
      "epoch 1 / 2, step  600/600, loss 0.44005\n",
      "epoch 2 / 2, step  100/600, loss 0.22205\n",
      "epoch 2 / 2, step  200/600, loss 0.16783\n",
      "epoch 2 / 2, step  300/600, loss 0.14191\n",
      "epoch 2 / 2, step  400/600, loss 0.18785\n",
      "epoch 2 / 2, step  500/600, loss 0.15795\n",
      "epoch 2 / 2, step  600/600, loss 0.11088\n"
     ]
    }
   ],
   "source": [
    "n_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, sequence_length, input_size)\n",
    "        \n",
    "        pred = model(images)\n",
    "        loss = criteria(pred, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 99:\n",
    "            print(f'epoch {epoch+1} / {nepochs}, step {i+1:4d}/{n_total_steps}, loss {loss.item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    samples = 0 \n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size)\n",
    "        pred = model(images)\n",
    "        \n",
    "        _, predictions = torch.max(pred, 1)\n",
    "        samples += labels.shape[0]\n",
    "        correct += (predictions == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acurracy: 93.69%\n"
     ]
    }
   ],
   "source": [
    "print(f'Test acurracy: {correct / samples * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 2, step  100/600, loss 0.65089\n",
      "epoch 1 / 2, step  200/600, loss 0.51852\n",
      "epoch 1 / 2, step  300/600, loss 0.14302\n",
      "epoch 1 / 2, step  400/600, loss 0.24724\n",
      "epoch 1 / 2, step  500/600, loss 0.14750\n",
      "epoch 1 / 2, step  600/600, loss 0.15972\n",
      "epoch 2 / 2, step  100/600, loss 0.11752\n",
      "epoch 2 / 2, step  200/600, loss 0.08276\n",
      "epoch 2 / 2, step  300/600, loss 0.04663\n",
      "epoch 2 / 2, step  400/600, loss 0.11340\n",
      "epoch 2 / 2, step  500/600, loss 0.02573\n",
      "epoch 2 / 2, step  600/600, loss 0.03570\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size \n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "model = RNN(input_size, hidden_size, n_layers, num_classes)\n",
    "\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, sequence_length, input_size)\n",
    "        \n",
    "        pred = model(images)\n",
    "        loss = criteria(pred, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 99:\n",
    "            print(f'epoch {epoch+1} / {nepochs}, step {i+1:4d}/{n_total_steps}, loss {loss.item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acurracy: 97.80%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    samples = 0 \n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size)\n",
    "        pred = model(images)\n",
    "        \n",
    "        _, predictions = torch.max(pred, 1)\n",
    "        samples += labels.shape[0]\n",
    "        correct += (predictions == labels).sum().item()\n",
    "\n",
    "print(f'Test acurracy: {correct / samples * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 2, step  100/600, loss 0.66581\n",
      "epoch 1 / 2, step  200/600, loss 0.45814\n",
      "epoch 1 / 2, step  300/600, loss 0.16653\n",
      "epoch 1 / 2, step  400/600, loss 0.24277\n",
      "epoch 1 / 2, step  500/600, loss 0.12623\n",
      "epoch 1 / 2, step  600/600, loss 0.12943\n",
      "epoch 2 / 2, step  100/600, loss 0.25156\n",
      "epoch 2 / 2, step  200/600, loss 0.11204\n",
      "epoch 2 / 2, step  300/600, loss 0.06191\n",
      "epoch 2 / 2, step  400/600, loss 0.18741\n",
      "epoch 2 / 2, step  500/600, loss 0.06041\n",
      "epoch 2 / 2, step  600/600, loss 0.09884\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size \n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.rnn(x, (h0, c0))\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "model = RNN(input_size, hidden_size, n_layers, num_classes)\n",
    "\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, sequence_length, input_size)\n",
    "        \n",
    "        pred = model(images)\n",
    "        loss = criteria(pred, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 99:\n",
    "            print(f'epoch {epoch+1} / {nepochs}, step {i+1:4d}/{n_total_steps}, loss {loss.item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acurracy: 94.93%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    samples = 0 \n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size)\n",
    "        pred = model(images)\n",
    "        \n",
    "        _, predictions = torch.max(pred, 1)\n",
    "        samples += labels.shape[0]\n",
    "        correct += (predictions == labels).sum().item()\n",
    "\n",
    "print(f'Test acurracy: {correct / samples * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9784455f3e9463d14dc3263833cb7a66fe8439c0a7d698fd0d368a30d65f5d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
