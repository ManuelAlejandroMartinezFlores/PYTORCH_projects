{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision as tv \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import numpy as np\n",
    "import math \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('wine.csv', delimiter=\",\", dtype=np.float32, skiprows=1)\n",
    "        sc = StandardScaler()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(xy[:, 1:], xy[:, 0] - 1, test_size=0.2)\n",
    "        self.x = torch.from_numpy(sc.fit_transform(x_train))\n",
    "        self.y = torch.from_numpy(y_train.astype(np.int64))\n",
    "        self.x_test = torch.from_numpy(x_test)\n",
    "        self.y_test = torch.from_numpy(y_test.astype(np.int64))\n",
    "        self.n_samples = x_train.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1740, -0.6836,  0.5017, -0.5680, -0.3358,  0.3373,  0.3946, -0.8845,\n",
       "         -0.1909, -0.5032,  0.6587,  1.4392,  0.8622]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = WineDataset()\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.7762, -0.6315, -0.0685, -0.1759,  0.4390,  0.9562,  1.2127, -1.2004,\n",
       "           0.6498,  0.7323,  0.6587,  0.4121,  2.4561],\n",
       "         [-0.8508, -1.2133, -1.5298, -1.4728,  2.5520, -0.6073, -0.1242, -0.1736,\n",
       "           2.0396, -0.7237,  0.5244, -0.3685,  0.0175],\n",
       "         [-0.1740,  0.7667,  0.7156,  0.6988,  0.4390, -1.0144, -1.3714,  1.8012,\n",
       "          -1.0488,  0.1832, -0.3264, -0.6424, -0.5562],\n",
       "         [-0.3432, -0.5533, -0.3536,  0.8496, -1.1106, -1.4541, -0.2140,  0.8533,\n",
       "           0.1008, -0.7653, -0.2816, -0.2179, -0.8176]]),\n",
       " tensor([0, 1, 2, 1])]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(dataloader)\n",
    "data = dataiter.next()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nepochs = 2000\n",
    "samples = len(dataset)\n",
    "n_it = math.ceil(samples/4)\n",
    "n_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class WineModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(WineModel, self).__init__()\n",
    "        self.d1 = nn.Linear(input_size, 32)\n",
    "        self.d2 = nn.Linear(32, 32)\n",
    "        self.d3 = nn.Linear(32, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.d1(x))\n",
    "        x = nn.functional.relu(self.d2(x))\n",
    "        # return nn.functional.relu(self.d3(x))\n",
    "        return self.d3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WineModel(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = nn.CrossEntropyLoss()\n",
    "lr = 0.00001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/2000, loss = 1.0940611362, val loss = 16.2694950104\n",
      "epoch: 2/2000, loss = 1.2336566448, val loss = 16.2392272949\n",
      "epoch: 3/2000, loss = 1.1578824520, val loss = 16.2104530334\n",
      "epoch: 4/2000, loss = 1.1612272263, val loss = 16.1825561523\n",
      "epoch: 5/2000, loss = 1.0096404552, val loss = 16.1547966003\n",
      "epoch: 6/2000, loss = 1.0306637287, val loss = 16.1264152527\n",
      "epoch: 7/2000, loss = 1.2314867973, val loss = 16.0968799591\n",
      "epoch: 8/2000, loss = 1.2098596096, val loss = 16.0685939789\n",
      "epoch: 9/2000, loss = 1.2724972963, val loss = 16.0388317108\n",
      "epoch: 10/2000, loss = 1.0829541683, val loss = 16.0106830597\n",
      "epoch: 11/2000, loss = 1.1036411524, val loss = 15.9830760956\n",
      "epoch: 12/2000, loss = 1.0708961487, val loss = 15.9554796219\n",
      "epoch: 13/2000, loss = 1.1565721035, val loss = 15.9274187088\n",
      "epoch: 14/2000, loss = 1.0852992535, val loss = 15.8999433517\n",
      "epoch: 15/2000, loss = 0.9761976004, val loss = 15.8727331161\n",
      "epoch: 16/2000, loss = 1.1215239763, val loss = 15.8445262909\n",
      "epoch: 17/2000, loss = 1.1540108919, val loss = 15.8162794113\n",
      "epoch: 18/2000, loss = 1.1559660435, val loss = 15.7869396210\n",
      "epoch: 19/2000, loss = 1.1829714775, val loss = 15.7576618195\n",
      "epoch: 20/2000, loss = 0.9117093086, val loss = 15.7308912277\n",
      "epoch: 21/2000, loss = 1.1594779491, val loss = 15.7030296326\n",
      "epoch: 22/2000, loss = 1.0669858456, val loss = 15.6754999161\n",
      "epoch: 23/2000, loss = 1.1659922600, val loss = 15.6463212967\n",
      "epoch: 24/2000, loss = 1.0698895454, val loss = 15.6192588806\n",
      "epoch: 25/2000, loss = 1.1735746861, val loss = 15.5914325714\n",
      "epoch: 26/2000, loss = 1.2552213669, val loss = 15.5604991913\n",
      "epoch: 27/2000, loss = 1.2299401760, val loss = 15.5310649872\n",
      "epoch: 28/2000, loss = 1.0692273378, val loss = 15.5036048889\n",
      "epoch: 29/2000, loss = 1.1966729164, val loss = 15.4747047424\n",
      "epoch: 30/2000, loss = 1.2097735405, val loss = 15.4462604523\n",
      "epoch: 31/2000, loss = 1.0577707291, val loss = 15.4191389084\n",
      "epoch: 32/2000, loss = 1.0809338093, val loss = 15.3916864395\n",
      "epoch: 33/2000, loss = 1.1404267550, val loss = 15.3633050919\n",
      "epoch: 34/2000, loss = 1.1466903687, val loss = 15.3358936310\n",
      "epoch: 35/2000, loss = 1.0396896601, val loss = 15.3088340759\n",
      "epoch: 36/2000, loss = 1.1694636345, val loss = 15.2808227539\n",
      "epoch: 37/2000, loss = 1.0488576889, val loss = 15.2540397644\n",
      "epoch: 38/2000, loss = 1.1941230297, val loss = 15.2245702744\n",
      "epoch: 39/2000, loss = 1.1465251446, val loss = 15.1960058212\n",
      "epoch: 40/2000, loss = 1.0868229866, val loss = 15.1686792374\n",
      "epoch: 41/2000, loss = 1.1938774586, val loss = 15.1400346756\n",
      "epoch: 42/2000, loss = 1.1075904369, val loss = 15.1125926971\n",
      "epoch: 43/2000, loss = 1.1666743755, val loss = 15.0836286545\n",
      "epoch: 44/2000, loss = 1.0110392570, val loss = 15.0565557480\n",
      "epoch: 45/2000, loss = 1.2031478882, val loss = 15.0279579163\n",
      "epoch: 46/2000, loss = 1.0931527615, val loss = 15.0007734299\n",
      "epoch: 47/2000, loss = 1.0372846127, val loss = 14.9733123779\n",
      "epoch: 48/2000, loss = 1.0440506935, val loss = 14.9464073181\n",
      "epoch: 49/2000, loss = 1.1251561642, val loss = 14.9187097549\n",
      "epoch: 50/2000, loss = 1.1020402908, val loss = 14.8912248611\n",
      "epoch: 51/2000, loss = 1.1609694958, val loss = 14.8619909286\n",
      "epoch: 52/2000, loss = 1.1370891333, val loss = 14.8343267441\n",
      "epoch: 53/2000, loss = 1.1642560959, val loss = 14.8062028885\n",
      "epoch: 54/2000, loss = 1.1647101641, val loss = 14.7784252167\n",
      "epoch: 55/2000, loss = 1.2257909775, val loss = 14.7497234344\n",
      "epoch: 56/2000, loss = 1.1080347300, val loss = 14.7222700119\n",
      "epoch: 57/2000, loss = 1.1368291378, val loss = 14.6941108704\n",
      "epoch: 58/2000, loss = 1.0677769184, val loss = 14.6667394638\n",
      "epoch: 59/2000, loss = 1.0922396183, val loss = 14.6392583847\n",
      "epoch: 60/2000, loss = 1.2081768513, val loss = 14.6109790802\n",
      "epoch: 61/2000, loss = 0.9887692928, val loss = 14.5839300156\n",
      "epoch: 62/2000, loss = 1.2087392807, val loss = 14.5559558868\n",
      "epoch: 63/2000, loss = 1.1300013065, val loss = 14.5286149979\n",
      "epoch: 64/2000, loss = 1.2151024342, val loss = 14.4993000031\n",
      "epoch: 65/2000, loss = 1.1555817127, val loss = 14.4716405869\n",
      "epoch: 66/2000, loss = 1.1927729845, val loss = 14.4428672791\n",
      "epoch: 67/2000, loss = 1.2130594254, val loss = 14.4144525528\n",
      "epoch: 68/2000, loss = 1.1498661041, val loss = 14.3861303329\n",
      "epoch: 69/2000, loss = 1.2381584644, val loss = 14.3568620682\n",
      "epoch: 70/2000, loss = 1.1027772427, val loss = 14.3296318054\n",
      "epoch: 71/2000, loss = 1.2105032206, val loss = 14.3001775742\n",
      "epoch: 72/2000, loss = 1.1020600796, val loss = 14.2727336884\n",
      "epoch: 73/2000, loss = 1.0044630766, val loss = 14.2459154129\n",
      "epoch: 74/2000, loss = 1.1989080906, val loss = 14.2171583176\n",
      "epoch: 75/2000, loss = 1.0118107796, val loss = 14.1902008057\n",
      "epoch: 76/2000, loss = 1.0257948637, val loss = 14.1631937027\n",
      "epoch: 77/2000, loss = 1.0946063995, val loss = 14.1353979111\n",
      "epoch: 78/2000, loss = 1.1320586205, val loss = 14.1073064804\n",
      "epoch: 79/2000, loss = 0.9572465420, val loss = 14.0803041458\n",
      "epoch: 80/2000, loss = 0.9407541156, val loss = 14.0533685684\n",
      "epoch: 81/2000, loss = 1.1804661751, val loss = 14.0256204605\n",
      "epoch: 82/2000, loss = 1.2238303423, val loss = 13.9974412918\n",
      "epoch: 83/2000, loss = 0.9152141213, val loss = 13.9705924988\n",
      "epoch: 84/2000, loss = 1.2517080307, val loss = 13.9421310425\n",
      "epoch: 85/2000, loss = 1.1060717106, val loss = 13.9141826630\n",
      "epoch: 86/2000, loss = 1.0247228146, val loss = 13.8870458603\n",
      "epoch: 87/2000, loss = 1.0337269306, val loss = 13.8601026535\n",
      "epoch: 88/2000, loss = 1.1552565098, val loss = 13.8320026398\n",
      "epoch: 89/2000, loss = 1.0529654026, val loss = 13.8053627014\n",
      "epoch: 90/2000, loss = 1.0776258707, val loss = 13.7782936096\n",
      "epoch: 91/2000, loss = 1.1567889452, val loss = 13.7504634857\n",
      "epoch: 92/2000, loss = 1.0709108114, val loss = 13.7233381271\n",
      "epoch: 93/2000, loss = 0.9799876213, val loss = 13.6966953278\n",
      "epoch: 94/2000, loss = 1.1658884287, val loss = 13.6686172485\n",
      "epoch: 95/2000, loss = 1.0617599487, val loss = 13.6414279938\n",
      "epoch: 96/2000, loss = 1.1871091127, val loss = 13.6131162643\n",
      "epoch: 97/2000, loss = 1.1864545345, val loss = 13.5839881897\n",
      "epoch: 98/2000, loss = 1.1598753929, val loss = 13.5555410385\n",
      "epoch: 99/2000, loss = 1.0781998634, val loss = 13.5273704529\n",
      "epoch: 100/2000, loss = 1.1959478855, val loss = 13.4986953735\n",
      "epoch: 101/2000, loss = 1.1010975838, val loss = 13.4716062546\n",
      "epoch: 102/2000, loss = 1.1238281727, val loss = 13.4435005188\n",
      "epoch: 103/2000, loss = 1.2021620274, val loss = 13.4139480591\n",
      "epoch: 104/2000, loss = 1.1613583565, val loss = 13.3855514526\n",
      "epoch: 105/2000, loss = 1.0672986507, val loss = 13.3576164246\n",
      "epoch: 106/2000, loss = 1.1105706692, val loss = 13.3299684525\n",
      "epoch: 107/2000, loss = 1.1018328667, val loss = 13.3029136658\n",
      "epoch: 108/2000, loss = 0.9620450735, val loss = 13.2762184143\n",
      "epoch: 109/2000, loss = 1.0704405308, val loss = 13.2496137619\n",
      "epoch: 110/2000, loss = 1.1065683365, val loss = 13.2225198746\n",
      "epoch: 111/2000, loss = 1.1375966072, val loss = 13.1948537827\n",
      "epoch: 112/2000, loss = 1.1979584694, val loss = 13.1664981842\n",
      "epoch: 113/2000, loss = 0.9272760153, val loss = 13.1400308609\n",
      "epoch: 114/2000, loss = 1.2171831131, val loss = 13.1113996506\n",
      "epoch: 115/2000, loss = 1.2155222893, val loss = 13.0819187164\n",
      "epoch: 116/2000, loss = 1.0205676556, val loss = 13.0551137924\n",
      "epoch: 117/2000, loss = 1.1039209366, val loss = 13.0274591446\n",
      "epoch: 118/2000, loss = 1.0705542564, val loss = 12.9995622635\n",
      "epoch: 119/2000, loss = 0.9449174404, val loss = 12.9728136063\n",
      "epoch: 120/2000, loss = 1.0541063547, val loss = 12.9457378387\n",
      "epoch: 121/2000, loss = 1.0901451111, val loss = 12.9186229706\n",
      "epoch: 122/2000, loss = 1.1288990974, val loss = 12.8912401199\n",
      "epoch: 123/2000, loss = 1.1517912149, val loss = 12.8639307022\n",
      "epoch: 124/2000, loss = 1.0994282961, val loss = 12.8367786407\n",
      "epoch: 125/2000, loss = 1.1205898523, val loss = 12.8088893890\n",
      "epoch: 126/2000, loss = 1.0158867836, val loss = 12.7821626663\n",
      "epoch: 127/2000, loss = 1.1416692734, val loss = 12.7548990250\n",
      "epoch: 128/2000, loss = 1.1638777256, val loss = 12.7265882492\n",
      "epoch: 129/2000, loss = 1.0989496708, val loss = 12.6997947693\n",
      "epoch: 130/2000, loss = 1.0575641394, val loss = 12.6728353500\n",
      "epoch: 131/2000, loss = 1.0831725597, val loss = 12.6457719803\n",
      "epoch: 132/2000, loss = 1.1697605848, val loss = 12.6176271439\n",
      "epoch: 133/2000, loss = 1.0788627863, val loss = 12.5906200409\n",
      "epoch: 134/2000, loss = 1.0386708975, val loss = 12.5637769699\n",
      "epoch: 135/2000, loss = 1.2343037128, val loss = 12.5359611511\n",
      "epoch: 136/2000, loss = 0.9677702188, val loss = 12.5096473694\n",
      "epoch: 137/2000, loss = 1.1512531042, val loss = 12.4818019867\n",
      "epoch: 138/2000, loss = 1.2014231682, val loss = 12.4529371262\n",
      "epoch: 139/2000, loss = 1.0858870745, val loss = 12.4256486893\n",
      "epoch: 140/2000, loss = 0.9838538766, val loss = 12.3990297318\n",
      "epoch: 141/2000, loss = 1.1722240448, val loss = 12.3715095520\n",
      "epoch: 142/2000, loss = 1.0794843435, val loss = 12.3446254730\n",
      "epoch: 143/2000, loss = 1.1346440315, val loss = 12.3173704147\n",
      "epoch: 144/2000, loss = 1.0389362574, val loss = 12.2904100418\n",
      "epoch: 145/2000, loss = 1.1153740883, val loss = 12.2625913620\n",
      "epoch: 146/2000, loss = 1.1922757626, val loss = 12.2347679138\n",
      "epoch: 147/2000, loss = 1.1003304720, val loss = 12.2080450058\n",
      "epoch: 148/2000, loss = 1.0963830948, val loss = 12.1811113358\n",
      "epoch: 149/2000, loss = 0.9587072730, val loss = 12.1546707153\n",
      "epoch: 150/2000, loss = 1.1842213869, val loss = 12.1261329651\n",
      "epoch: 151/2000, loss = 1.1757330894, val loss = 12.0979509354\n",
      "epoch: 152/2000, loss = 1.1779800653, val loss = 12.0699386597\n",
      "epoch: 153/2000, loss = 1.2263708115, val loss = 12.0425224304\n",
      "epoch: 154/2000, loss = 1.0442967415, val loss = 12.0162601471\n",
      "epoch: 155/2000, loss = 1.1951004267, val loss = 11.9879722595\n",
      "epoch: 156/2000, loss = 0.9957060814, val loss = 11.9615411758\n",
      "epoch: 157/2000, loss = 1.0524134636, val loss = 11.9346656799\n",
      "epoch: 158/2000, loss = 1.0889415741, val loss = 11.9075174332\n",
      "epoch: 159/2000, loss = 1.1950988770, val loss = 11.8784246445\n",
      "epoch: 160/2000, loss = 1.1385790110, val loss = 11.8505849838\n",
      "epoch: 161/2000, loss = 1.1206228733, val loss = 11.8233270645\n",
      "epoch: 162/2000, loss = 1.1494612694, val loss = 11.7961940765\n",
      "epoch: 163/2000, loss = 1.0736657381, val loss = 11.7692365646\n",
      "epoch: 164/2000, loss = 1.1343472004, val loss = 11.7410783768\n",
      "epoch: 165/2000, loss = 1.1732285023, val loss = 11.7133131027\n",
      "epoch: 166/2000, loss = 1.1051555872, val loss = 11.6864995956\n",
      "epoch: 167/2000, loss = 1.2217235565, val loss = 11.6579303741\n",
      "epoch: 168/2000, loss = 1.0945084095, val loss = 11.6307535172\n",
      "epoch: 169/2000, loss = 1.1952195168, val loss = 11.6024894714\n",
      "epoch: 170/2000, loss = 1.2389545441, val loss = 11.5743560791\n",
      "epoch: 171/2000, loss = 1.0721566677, val loss = 11.5475463867\n",
      "epoch: 172/2000, loss = 1.2082674503, val loss = 11.5188226700\n",
      "epoch: 173/2000, loss = 1.1632051468, val loss = 11.4907331467\n",
      "epoch: 174/2000, loss = 1.2262370586, val loss = 11.4621896744\n",
      "epoch: 175/2000, loss = 1.0549261570, val loss = 11.4347190857\n",
      "epoch: 176/2000, loss = 1.1724138260, val loss = 11.4067602158\n",
      "epoch: 177/2000, loss = 1.1324365139, val loss = 11.3792076111\n",
      "epoch: 178/2000, loss = 1.0644668341, val loss = 11.3522768021\n",
      "epoch: 179/2000, loss = 1.2000942230, val loss = 11.3242473602\n",
      "epoch: 180/2000, loss = 1.1892466545, val loss = 11.2964286804\n",
      "epoch: 181/2000, loss = 1.1284277439, val loss = 11.2698917389\n",
      "epoch: 182/2000, loss = 1.0894465446, val loss = 11.2412595749\n",
      "epoch: 183/2000, loss = 0.9549459219, val loss = 11.2150335312\n",
      "epoch: 184/2000, loss = 1.1339758635, val loss = 11.1874837875\n",
      "epoch: 185/2000, loss = 1.1702044010, val loss = 11.1596384048\n",
      "epoch: 186/2000, loss = 1.0281528234, val loss = 11.1327610016\n",
      "epoch: 187/2000, loss = 1.2659530640, val loss = 11.1042137146\n",
      "epoch: 188/2000, loss = 1.0593504906, val loss = 11.0769042969\n",
      "epoch: 189/2000, loss = 0.9490252137, val loss = 11.0503530502\n",
      "epoch: 190/2000, loss = 1.1609051228, val loss = 11.0220527649\n",
      "epoch: 191/2000, loss = 1.2595423460, val loss = 10.9934730530\n",
      "epoch: 192/2000, loss = 1.1976490021, val loss = 10.9641504288\n",
      "epoch: 193/2000, loss = 1.2535452843, val loss = 10.9360427856\n",
      "epoch: 194/2000, loss = 1.1437883377, val loss = 10.9085083008\n",
      "epoch: 195/2000, loss = 1.1159206629, val loss = 10.8807725906\n",
      "epoch: 196/2000, loss = 1.1481455564, val loss = 10.8537034988\n",
      "epoch: 197/2000, loss = 1.1528043747, val loss = 10.8260765076\n",
      "epoch: 198/2000, loss = 1.1281223297, val loss = 10.7989845276\n",
      "epoch: 199/2000, loss = 1.1191828251, val loss = 10.7722549438\n",
      "epoch: 200/2000, loss = 1.1297800541, val loss = 10.7437782288\n",
      "epoch: 201/2000, loss = 1.0392013788, val loss = 10.7170438766\n",
      "epoch: 202/2000, loss = 1.1767132282, val loss = 10.6893453598\n",
      "epoch: 203/2000, loss = 1.0699298382, val loss = 10.6616096497\n",
      "epoch: 204/2000, loss = 1.1784050465, val loss = 10.6331958771\n",
      "epoch: 205/2000, loss = 1.1925230026, val loss = 10.6052942276\n",
      "epoch: 206/2000, loss = 1.1888296604, val loss = 10.5764942169\n",
      "epoch: 207/2000, loss = 1.1736755371, val loss = 10.5488739014\n",
      "epoch: 208/2000, loss = 1.0593925714, val loss = 10.5223045349\n",
      "epoch: 209/2000, loss = 1.0846645832, val loss = 10.4956130981\n",
      "epoch: 210/2000, loss = 0.9893652201, val loss = 10.4690732956\n",
      "epoch: 211/2000, loss = 1.1124017239, val loss = 10.4410972595\n",
      "epoch: 212/2000, loss = 1.1330192089, val loss = 10.4140920639\n",
      "epoch: 213/2000, loss = 1.1555614471, val loss = 10.3860750198\n",
      "epoch: 214/2000, loss = 0.9932440519, val loss = 10.3597526550\n",
      "epoch: 215/2000, loss = 1.0684865713, val loss = 10.3322486877\n",
      "epoch: 216/2000, loss = 1.1910774708, val loss = 10.3045730591\n",
      "epoch: 217/2000, loss = 1.0867496729, val loss = 10.2768974304\n",
      "epoch: 218/2000, loss = 1.1344895363, val loss = 10.2496833801\n",
      "epoch: 219/2000, loss = 1.0437269211, val loss = 10.2232789993\n",
      "epoch: 220/2000, loss = 1.1372847557, val loss = 10.1965675354\n",
      "epoch: 221/2000, loss = 0.9868525863, val loss = 10.1702051163\n",
      "epoch: 222/2000, loss = 1.1885521412, val loss = 10.1418895721\n",
      "epoch: 223/2000, loss = 1.1558932066, val loss = 10.1143980026\n",
      "epoch: 224/2000, loss = 1.0897983313, val loss = 10.0869092941\n",
      "epoch: 225/2000, loss = 1.1909961700, val loss = 10.0580387115\n",
      "epoch: 226/2000, loss = 1.1166011095, val loss = 10.0308876038\n",
      "epoch: 227/2000, loss = 1.0785350800, val loss = 10.0045957565\n",
      "epoch: 228/2000, loss = 1.1661899090, val loss = 9.9777030945\n",
      "epoch: 229/2000, loss = 0.9654887915, val loss = 9.9515428543\n",
      "epoch: 230/2000, loss = 1.2320373058, val loss = 9.9224634171\n",
      "epoch: 231/2000, loss = 1.0915935040, val loss = 9.8958864212\n",
      "epoch: 232/2000, loss = 1.1216822863, val loss = 9.8684034348\n",
      "epoch: 233/2000, loss = 1.0680674314, val loss = 9.8417530060\n",
      "epoch: 234/2000, loss = 1.0932905674, val loss = 9.8151893616\n",
      "epoch: 235/2000, loss = 1.1348187923, val loss = 9.7882871628\n",
      "epoch: 236/2000, loss = 1.1615759134, val loss = 9.7614383698\n",
      "epoch: 237/2000, loss = 1.0670710802, val loss = 9.7343339920\n",
      "epoch: 238/2000, loss = 1.2470648289, val loss = 9.7057046890\n",
      "epoch: 239/2000, loss = 1.1507153511, val loss = 9.6774253845\n",
      "epoch: 240/2000, loss = 1.1624209881, val loss = 9.6489706039\n",
      "epoch: 241/2000, loss = 1.1580821276, val loss = 9.6221914291\n",
      "epoch: 242/2000, loss = 0.9651205540, val loss = 9.5960817337\n",
      "epoch: 243/2000, loss = 1.0301859379, val loss = 9.5696849823\n",
      "epoch: 244/2000, loss = 1.1852059364, val loss = 9.5418100357\n",
      "epoch: 245/2000, loss = 1.0735117197, val loss = 9.5144405365\n",
      "epoch: 246/2000, loss = 1.0885387659, val loss = 9.4879779816\n",
      "epoch: 247/2000, loss = 1.2119015455, val loss = 9.4601078033\n",
      "epoch: 248/2000, loss = 1.0556398630, val loss = 9.4337263107\n",
      "epoch: 249/2000, loss = 1.1626961231, val loss = 9.4064540863\n",
      "epoch: 250/2000, loss = 1.1843202114, val loss = 9.3783226013\n",
      "epoch: 251/2000, loss = 1.0946297646, val loss = 9.3518562317\n",
      "epoch: 252/2000, loss = 1.1467084885, val loss = 9.3241691589\n",
      "epoch: 253/2000, loss = 0.9808044434, val loss = 9.2979555130\n",
      "epoch: 254/2000, loss = 1.1398096085, val loss = 9.2715959549\n",
      "epoch: 255/2000, loss = 1.2017792463, val loss = 9.2434072495\n",
      "epoch: 256/2000, loss = 1.0822219849, val loss = 9.2161369324\n",
      "epoch: 257/2000, loss = 1.0868086815, val loss = 9.1897811890\n",
      "epoch: 258/2000, loss = 1.1107133627, val loss = 9.1630754471\n",
      "epoch: 259/2000, loss = 1.1695486307, val loss = 9.1358785629\n",
      "epoch: 260/2000, loss = 1.1129713058, val loss = 9.1089820862\n",
      "epoch: 261/2000, loss = 1.2026398182, val loss = 9.0812139511\n",
      "epoch: 262/2000, loss = 0.9757996202, val loss = 9.0553989410\n",
      "epoch: 263/2000, loss = 1.0303151608, val loss = 9.0289831161\n",
      "epoch: 264/2000, loss = 0.8953676224, val loss = 9.0030984879\n",
      "epoch: 265/2000, loss = 1.1597273350, val loss = 8.9761877060\n",
      "epoch: 266/2000, loss = 1.1060594320, val loss = 8.9494237900\n",
      "epoch: 267/2000, loss = 1.2895979881, val loss = 8.9209585190\n",
      "epoch: 268/2000, loss = 1.1208943129, val loss = 8.8942871094\n",
      "epoch: 269/2000, loss = 1.1715815067, val loss = 8.8665952682\n",
      "epoch: 270/2000, loss = 1.0356420279, val loss = 8.8405294418\n",
      "epoch: 271/2000, loss = 1.0298298597, val loss = 8.8137111664\n",
      "epoch: 272/2000, loss = 1.1561710835, val loss = 8.7866010666\n",
      "epoch: 273/2000, loss = 1.1772096157, val loss = 8.7580423355\n",
      "epoch: 274/2000, loss = 1.1204440594, val loss = 8.7308044434\n",
      "epoch: 275/2000, loss = 1.0838682652, val loss = 8.7048139572\n",
      "epoch: 276/2000, loss = 1.0819578171, val loss = 8.6784934998\n",
      "epoch: 277/2000, loss = 1.0343482494, val loss = 8.6523628235\n",
      "epoch: 278/2000, loss = 1.0330122709, val loss = 8.6263351440\n",
      "epoch: 279/2000, loss = 1.1009778976, val loss = 8.5993289948\n",
      "epoch: 280/2000, loss = 1.0846680403, val loss = 8.5718927383\n",
      "epoch: 281/2000, loss = 1.0628871918, val loss = 8.5455589294\n",
      "epoch: 282/2000, loss = 1.1581397057, val loss = 8.5187683105\n",
      "epoch: 283/2000, loss = 1.1248059273, val loss = 8.4911460876\n",
      "epoch: 284/2000, loss = 1.0520615578, val loss = 8.4650611877\n",
      "epoch: 285/2000, loss = 1.1327216625, val loss = 8.4384460449\n",
      "epoch: 286/2000, loss = 1.0943703651, val loss = 8.4109582901\n",
      "epoch: 287/2000, loss = 1.1666121483, val loss = 8.3840312958\n",
      "epoch: 288/2000, loss = 1.1752927303, val loss = 8.3574829102\n",
      "epoch: 289/2000, loss = 1.0181790590, val loss = 8.3312702179\n",
      "epoch: 290/2000, loss = 1.0498588085, val loss = 8.3050193787\n",
      "epoch: 291/2000, loss = 1.0805103779, val loss = 8.2780227661\n",
      "epoch: 292/2000, loss = 1.1500858068, val loss = 8.2510137558\n",
      "epoch: 293/2000, loss = 1.1762444973, val loss = 8.2242202759\n",
      "epoch: 294/2000, loss = 1.1621304750, val loss = 8.1973190308\n",
      "epoch: 295/2000, loss = 1.1533207893, val loss = 8.1699762344\n",
      "epoch: 296/2000, loss = 1.0571615696, val loss = 8.1437492371\n",
      "epoch: 297/2000, loss = 0.9969507456, val loss = 8.1177482605\n",
      "epoch: 298/2000, loss = 1.1460337639, val loss = 8.0912790298\n",
      "epoch: 299/2000, loss = 1.0494208336, val loss = 8.0650873184\n",
      "epoch: 300/2000, loss = 1.1280391216, val loss = 8.0386972427\n",
      "epoch: 301/2000, loss = 1.0221275091, val loss = 8.0120601654\n",
      "epoch: 302/2000, loss = 1.0326548815, val loss = 7.9865331650\n",
      "epoch: 303/2000, loss = 1.0219659805, val loss = 7.9608850479\n",
      "epoch: 304/2000, loss = 1.0171220303, val loss = 7.9350566864\n",
      "epoch: 305/2000, loss = 1.1013511419, val loss = 7.9086694717\n",
      "epoch: 306/2000, loss = 1.1057718992, val loss = 7.8817071915\n",
      "epoch: 307/2000, loss = 1.0879077911, val loss = 7.8553957939\n",
      "epoch: 308/2000, loss = 1.0934853554, val loss = 7.8294963837\n",
      "epoch: 309/2000, loss = 1.1023528576, val loss = 7.8036718369\n",
      "epoch: 310/2000, loss = 1.1222140789, val loss = 7.7766628265\n",
      "epoch: 311/2000, loss = 1.1534497738, val loss = 7.7495574951\n",
      "epoch: 312/2000, loss = 1.0394479036, val loss = 7.7236142159\n",
      "epoch: 313/2000, loss = 1.0264582634, val loss = 7.6977596283\n",
      "epoch: 314/2000, loss = 1.0500257015, val loss = 7.6708908081\n",
      "epoch: 315/2000, loss = 1.1569060087, val loss = 7.6438674927\n",
      "epoch: 316/2000, loss = 1.1074402332, val loss = 7.6178488731\n",
      "epoch: 317/2000, loss = 1.0690741539, val loss = 7.5923485756\n",
      "epoch: 318/2000, loss = 1.0231180191, val loss = 7.5663723946\n",
      "epoch: 319/2000, loss = 1.1808667183, val loss = 7.5379190445\n",
      "epoch: 320/2000, loss = 1.1494939327, val loss = 7.5115847588\n",
      "epoch: 321/2000, loss = 1.0796482563, val loss = 7.4853296280\n",
      "epoch: 322/2000, loss = 1.1422724724, val loss = 7.4578347206\n",
      "epoch: 323/2000, loss = 1.0774424076, val loss = 7.4317378998\n",
      "epoch: 324/2000, loss = 1.1672663689, val loss = 7.4051427841\n",
      "epoch: 325/2000, loss = 1.1820819378, val loss = 7.3780646324\n",
      "epoch: 326/2000, loss = 1.1308114529, val loss = 7.3510003090\n",
      "epoch: 327/2000, loss = 1.1233832836, val loss = 7.3244609833\n",
      "epoch: 328/2000, loss = 1.0289795399, val loss = 7.2985820770\n",
      "epoch: 329/2000, loss = 1.0863115788, val loss = 7.2726922035\n",
      "epoch: 330/2000, loss = 1.0149915218, val loss = 7.2470149994\n",
      "epoch: 331/2000, loss = 1.1425558329, val loss = 7.2194027901\n",
      "epoch: 332/2000, loss = 1.1759763956, val loss = 7.1914248466\n",
      "epoch: 333/2000, loss = 1.0483812094, val loss = 7.1653451920\n",
      "epoch: 334/2000, loss = 1.0921840668, val loss = 7.1388125420\n",
      "epoch: 335/2000, loss = 1.0666018724, val loss = 7.1128201485\n",
      "epoch: 336/2000, loss = 1.1520419121, val loss = 7.0866465569\n",
      "epoch: 337/2000, loss = 0.9540952444, val loss = 7.0610852242\n",
      "epoch: 338/2000, loss = 1.1804646254, val loss = 7.0346164703\n",
      "epoch: 339/2000, loss = 1.1825461388, val loss = 7.0071163177\n",
      "epoch: 340/2000, loss = 1.2043414116, val loss = 6.9800224304\n",
      "epoch: 341/2000, loss = 1.1508464813, val loss = 6.9526314735\n",
      "epoch: 342/2000, loss = 1.0232893229, val loss = 6.9262747765\n",
      "epoch: 343/2000, loss = 1.2041710615, val loss = 6.8981914520\n",
      "epoch: 344/2000, loss = 1.0982704163, val loss = 6.8723173141\n",
      "epoch: 345/2000, loss = 1.0483582020, val loss = 6.8465642929\n",
      "epoch: 346/2000, loss = 1.0011768341, val loss = 6.8212981224\n",
      "epoch: 347/2000, loss = 1.2112853527, val loss = 6.7942061424\n",
      "epoch: 348/2000, loss = 1.0404496193, val loss = 6.7683701515\n",
      "epoch: 349/2000, loss = 1.1561794281, val loss = 6.7394962311\n",
      "epoch: 350/2000, loss = 1.1145105362, val loss = 6.7139768600\n",
      "epoch: 351/2000, loss = 1.0861346722, val loss = 6.6874203682\n",
      "epoch: 352/2000, loss = 1.0632474422, val loss = 6.6616392136\n",
      "epoch: 353/2000, loss = 1.1197094917, val loss = 6.6352338791\n",
      "epoch: 354/2000, loss = 1.1882882118, val loss = 6.6077003479\n",
      "epoch: 355/2000, loss = 1.0331556797, val loss = 6.5811119080\n",
      "epoch: 356/2000, loss = 1.0810222626, val loss = 6.5553817749\n",
      "epoch: 357/2000, loss = 1.0322666168, val loss = 6.5298819542\n",
      "epoch: 358/2000, loss = 1.1040903330, val loss = 6.5038704872\n",
      "epoch: 359/2000, loss = 1.0360369682, val loss = 6.4783926010\n",
      "epoch: 360/2000, loss = 1.0902782679, val loss = 6.4517345428\n",
      "epoch: 361/2000, loss = 1.0358128548, val loss = 6.4266414642\n",
      "epoch: 362/2000, loss = 1.0531911850, val loss = 6.4005541801\n",
      "epoch: 363/2000, loss = 1.1768687963, val loss = 6.3736438751\n",
      "epoch: 364/2000, loss = 1.1666644812, val loss = 6.3470516205\n",
      "epoch: 365/2000, loss = 1.1647324562, val loss = 6.3210673332\n",
      "epoch: 366/2000, loss = 0.9960609674, val loss = 6.2958793640\n",
      "epoch: 367/2000, loss = 1.1831028461, val loss = 6.2687773705\n",
      "epoch: 368/2000, loss = 1.0606123209, val loss = 6.2422809601\n",
      "epoch: 369/2000, loss = 1.1279766560, val loss = 6.2158975601\n",
      "epoch: 370/2000, loss = 1.1850547791, val loss = 6.1890463829\n",
      "epoch: 371/2000, loss = 1.1389214993, val loss = 6.1637282372\n",
      "epoch: 372/2000, loss = 1.1707335711, val loss = 6.1367282867\n",
      "epoch: 373/2000, loss = 1.1289943457, val loss = 6.1108074188\n",
      "epoch: 374/2000, loss = 1.1196889877, val loss = 6.0839629173\n",
      "epoch: 375/2000, loss = 1.0996661186, val loss = 6.0580630302\n",
      "epoch: 376/2000, loss = 1.0937063694, val loss = 6.0318832397\n",
      "epoch: 377/2000, loss = 1.0994778872, val loss = 6.0055527687\n",
      "epoch: 378/2000, loss = 1.0631704330, val loss = 5.9799423218\n",
      "epoch: 379/2000, loss = 1.0804986954, val loss = 5.9544205666\n",
      "epoch: 380/2000, loss = 1.1943645477, val loss = 5.9284954071\n",
      "epoch: 381/2000, loss = 1.0025801659, val loss = 5.9036083221\n",
      "epoch: 382/2000, loss = 1.0625392199, val loss = 5.8782229424\n",
      "epoch: 383/2000, loss = 0.9319506884, val loss = 5.8533420563\n",
      "epoch: 384/2000, loss = 1.1431665421, val loss = 5.8279309273\n",
      "epoch: 385/2000, loss = 1.1952524185, val loss = 5.8016624451\n",
      "epoch: 386/2000, loss = 1.0695233345, val loss = 5.7757048607\n",
      "epoch: 387/2000, loss = 1.0986530781, val loss = 5.7505021095\n",
      "epoch: 388/2000, loss = 1.1398994923, val loss = 5.7239179611\n",
      "epoch: 389/2000, loss = 1.0055831671, val loss = 5.6988353729\n",
      "epoch: 390/2000, loss = 1.0304921865, val loss = 5.6735606194\n",
      "epoch: 391/2000, loss = 0.9607281685, val loss = 5.6484937668\n",
      "epoch: 392/2000, loss = 1.0400974751, val loss = 5.6230750084\n",
      "epoch: 393/2000, loss = 1.2031160593, val loss = 5.5968227386\n",
      "epoch: 394/2000, loss = 1.0917818546, val loss = 5.5712132454\n",
      "epoch: 395/2000, loss = 1.1476378441, val loss = 5.5455522537\n",
      "epoch: 396/2000, loss = 1.1562805176, val loss = 5.5190839767\n",
      "epoch: 397/2000, loss = 1.1321687698, val loss = 5.4922590256\n",
      "epoch: 398/2000, loss = 1.0860797167, val loss = 5.4666943550\n",
      "epoch: 399/2000, loss = 1.0787146091, val loss = 5.4407062531\n",
      "epoch: 400/2000, loss = 1.1472029686, val loss = 5.4151968956\n",
      "epoch: 401/2000, loss = 1.1580410004, val loss = 5.3879766464\n",
      "epoch: 402/2000, loss = 1.0363631248, val loss = 5.3628001213\n",
      "epoch: 403/2000, loss = 1.0080181360, val loss = 5.3375625610\n",
      "epoch: 404/2000, loss = 1.0446811914, val loss = 5.3123316765\n",
      "epoch: 405/2000, loss = 1.1322789192, val loss = 5.2865042686\n",
      "epoch: 406/2000, loss = 1.0779781342, val loss = 5.2606077194\n",
      "epoch: 407/2000, loss = 1.1861629486, val loss = 5.2333545685\n",
      "epoch: 408/2000, loss = 1.1316211224, val loss = 5.2079477310\n",
      "epoch: 409/2000, loss = 1.1524589062, val loss = 5.1816587448\n",
      "epoch: 410/2000, loss = 1.1007964611, val loss = 5.1557965279\n",
      "epoch: 411/2000, loss = 1.1142797470, val loss = 5.1309556961\n",
      "epoch: 412/2000, loss = 1.1669251919, val loss = 5.1050710678\n",
      "epoch: 413/2000, loss = 1.1163977385, val loss = 5.0789775848\n",
      "epoch: 414/2000, loss = 1.0786068439, val loss = 5.0540165901\n",
      "epoch: 415/2000, loss = 1.0909731388, val loss = 5.0290760994\n",
      "epoch: 416/2000, loss = 1.1671369076, val loss = 5.0026378632\n",
      "epoch: 417/2000, loss = 1.1456419230, val loss = 4.9774498940\n",
      "epoch: 418/2000, loss = 1.1073533297, val loss = 4.9523358345\n",
      "epoch: 419/2000, loss = 0.9930789471, val loss = 4.9281897545\n",
      "epoch: 420/2000, loss = 1.0759096146, val loss = 4.9020161629\n",
      "epoch: 421/2000, loss = 1.1131899357, val loss = 4.8768968582\n",
      "epoch: 422/2000, loss = 0.9844517708, val loss = 4.8522214890\n",
      "epoch: 423/2000, loss = 1.1309628487, val loss = 4.8262906075\n",
      "epoch: 424/2000, loss = 1.1313226223, val loss = 4.8002371788\n",
      "epoch: 425/2000, loss = 1.1852793694, val loss = 4.7748122215\n",
      "epoch: 426/2000, loss = 1.1763616800, val loss = 4.7491202354\n",
      "epoch: 427/2000, loss = 1.0791308880, val loss = 4.7236876488\n",
      "epoch: 428/2000, loss = 1.0633265972, val loss = 4.6991004944\n",
      "epoch: 429/2000, loss = 1.1753385067, val loss = 4.6725006104\n",
      "epoch: 430/2000, loss = 1.0862482786, val loss = 4.6475419998\n",
      "epoch: 431/2000, loss = 1.0489461422, val loss = 4.6229639053\n",
      "epoch: 432/2000, loss = 1.1621401310, val loss = 4.5981783867\n",
      "epoch: 433/2000, loss = 0.9784336686, val loss = 4.5737833977\n",
      "epoch: 434/2000, loss = 1.1965621710, val loss = 4.5484523773\n",
      "epoch: 435/2000, loss = 0.9601119757, val loss = 4.5246663094\n",
      "epoch: 436/2000, loss = 0.9248939753, val loss = 4.5006923676\n",
      "epoch: 437/2000, loss = 1.1686604023, val loss = 4.4749345779\n",
      "epoch: 438/2000, loss = 1.1180093288, val loss = 4.4493966103\n",
      "epoch: 439/2000, loss = 1.1743769646, val loss = 4.4240994453\n",
      "epoch: 440/2000, loss = 0.9573875666, val loss = 4.4002504349\n",
      "epoch: 441/2000, loss = 0.9960289001, val loss = 4.3762187958\n",
      "epoch: 442/2000, loss = 1.1700757742, val loss = 4.3495392799\n",
      "epoch: 443/2000, loss = 1.0406241417, val loss = 4.3255991936\n",
      "epoch: 444/2000, loss = 1.1231498718, val loss = 4.3008956909\n",
      "epoch: 445/2000, loss = 1.0099353790, val loss = 4.2771315575\n",
      "epoch: 446/2000, loss = 1.1880548000, val loss = 4.2521667480\n",
      "epoch: 447/2000, loss = 1.1338090897, val loss = 4.2282547951\n",
      "epoch: 448/2000, loss = 1.1956490278, val loss = 4.2034525871\n",
      "epoch: 449/2000, loss = 1.1298620701, val loss = 4.1789975166\n",
      "epoch: 450/2000, loss = 1.1521673203, val loss = 4.1549463272\n",
      "epoch: 451/2000, loss = 0.9769551158, val loss = 4.1316466331\n",
      "epoch: 452/2000, loss = 1.0736601353, val loss = 4.1079378128\n",
      "epoch: 453/2000, loss = 1.0106512308, val loss = 4.0844936371\n",
      "epoch: 454/2000, loss = 1.1257419586, val loss = 4.0598888397\n",
      "epoch: 455/2000, loss = 1.1457941532, val loss = 4.0360479355\n",
      "epoch: 456/2000, loss = 1.1068696976, val loss = 4.0125417709\n",
      "epoch: 457/2000, loss = 1.0740580559, val loss = 3.9881362915\n",
      "epoch: 458/2000, loss = 0.9089941978, val loss = 3.9653904438\n",
      "epoch: 459/2000, loss = 1.2182353735, val loss = 3.9407505989\n",
      "epoch: 460/2000, loss = 0.9453459978, val loss = 3.9182481766\n",
      "epoch: 461/2000, loss = 1.1612999439, val loss = 3.8941357136\n",
      "epoch: 462/2000, loss = 1.0054659843, val loss = 3.8710505962\n",
      "epoch: 463/2000, loss = 1.1310567856, val loss = 3.8474986553\n",
      "epoch: 464/2000, loss = 0.9550222754, val loss = 3.8248438835\n",
      "epoch: 465/2000, loss = 1.0765393972, val loss = 3.8011829853\n",
      "epoch: 466/2000, loss = 1.1347674131, val loss = 3.7779502869\n",
      "epoch: 467/2000, loss = 1.1452425718, val loss = 3.7533025742\n",
      "epoch: 468/2000, loss = 1.0223321915, val loss = 3.7297334671\n",
      "epoch: 469/2000, loss = 0.9545964003, val loss = 3.7074885368\n",
      "epoch: 470/2000, loss = 1.1154172421, val loss = 3.6843082905\n",
      "epoch: 471/2000, loss = 1.1922428608, val loss = 3.6600675583\n",
      "epoch: 472/2000, loss = 1.0226165056, val loss = 3.6377496719\n",
      "epoch: 473/2000, loss = 1.0808305740, val loss = 3.6153335571\n",
      "epoch: 474/2000, loss = 1.0389274359, val loss = 3.5931036472\n",
      "epoch: 475/2000, loss = 0.9871912003, val loss = 3.5711801052\n",
      "epoch: 476/2000, loss = 1.1531889439, val loss = 3.5485734940\n",
      "epoch: 477/2000, loss = 1.1760118008, val loss = 3.5248785019\n",
      "epoch: 478/2000, loss = 1.1974422932, val loss = 3.5021071434\n",
      "epoch: 479/2000, loss = 1.1206824780, val loss = 3.4795622826\n",
      "epoch: 480/2000, loss = 1.1721189022, val loss = 3.4569849968\n",
      "epoch: 481/2000, loss = 1.1497740746, val loss = 3.4349756241\n",
      "epoch: 482/2000, loss = 1.0929003954, val loss = 3.4133057594\n",
      "epoch: 483/2000, loss = 1.1300919056, val loss = 3.3906965256\n",
      "epoch: 484/2000, loss = 1.0960884094, val loss = 3.3680622578\n",
      "epoch: 485/2000, loss = 1.1566443443, val loss = 3.3459177017\n",
      "epoch: 486/2000, loss = 0.9927064776, val loss = 3.3238220215\n",
      "epoch: 487/2000, loss = 1.0333893299, val loss = 3.3026778698\n",
      "epoch: 488/2000, loss = 1.0526026487, val loss = 3.2816381454\n",
      "epoch: 489/2000, loss = 1.1785489321, val loss = 3.2588906288\n",
      "epoch: 490/2000, loss = 1.1685898304, val loss = 3.2370955944\n",
      "epoch: 491/2000, loss = 1.0317162275, val loss = 3.2163701057\n",
      "epoch: 492/2000, loss = 1.0331435204, val loss = 3.1955502033\n",
      "epoch: 493/2000, loss = 1.0210695267, val loss = 3.1752438545\n",
      "epoch: 494/2000, loss = 0.8771398067, val loss = 3.1552364826\n",
      "epoch: 495/2000, loss = 1.2687380314, val loss = 3.1335463524\n",
      "epoch: 496/2000, loss = 1.0389100313, val loss = 3.1133611202\n",
      "epoch: 497/2000, loss = 1.1292102337, val loss = 3.0931396484\n",
      "epoch: 498/2000, loss = 1.0194642544, val loss = 3.0732793808\n",
      "epoch: 499/2000, loss = 1.1517527103, val loss = 3.0526368618\n",
      "epoch: 500/2000, loss = 1.0444067717, val loss = 3.0323874950\n",
      "epoch: 501/2000, loss = 1.1630548239, val loss = 3.0117363930\n",
      "epoch: 502/2000, loss = 1.1350293159, val loss = 2.9922018051\n",
      "epoch: 503/2000, loss = 1.0908727646, val loss = 2.9723801613\n",
      "epoch: 504/2000, loss = 0.9784536362, val loss = 2.9529008865\n",
      "epoch: 505/2000, loss = 1.0947054625, val loss = 2.9336304665\n",
      "epoch: 506/2000, loss = 1.0877797604, val loss = 2.9150075912\n",
      "epoch: 507/2000, loss = 1.0927023888, val loss = 2.8962790966\n",
      "epoch: 508/2000, loss = 0.9664443731, val loss = 2.8784208298\n",
      "epoch: 509/2000, loss = 0.9991715550, val loss = 2.8596727848\n",
      "epoch: 510/2000, loss = 1.1214164495, val loss = 2.8411147594\n",
      "epoch: 511/2000, loss = 1.1436446905, val loss = 2.8220920563\n",
      "epoch: 512/2000, loss = 1.1378746033, val loss = 2.8030705452\n",
      "epoch: 513/2000, loss = 1.1343927383, val loss = 2.7843439579\n",
      "epoch: 514/2000, loss = 1.1092311144, val loss = 2.7656319141\n",
      "epoch: 515/2000, loss = 1.1451233625, val loss = 2.7467935085\n",
      "epoch: 516/2000, loss = 1.0804419518, val loss = 2.7290320396\n",
      "epoch: 517/2000, loss = 1.0218853951, val loss = 2.7114386559\n",
      "epoch: 518/2000, loss = 1.1707713604, val loss = 2.6933226585\n",
      "epoch: 519/2000, loss = 1.1765322685, val loss = 2.6763317585\n",
      "epoch: 520/2000, loss = 1.0788097382, val loss = 2.6589498520\n",
      "epoch: 521/2000, loss = 0.9801760912, val loss = 2.6431226730\n",
      "epoch: 522/2000, loss = 1.1509724855, val loss = 2.6264638901\n",
      "epoch: 523/2000, loss = 1.0931279659, val loss = 2.6095387936\n",
      "epoch: 524/2000, loss = 1.1296031475, val loss = 2.5929493904\n",
      "epoch: 525/2000, loss = 1.1175677776, val loss = 2.5764296055\n",
      "epoch: 526/2000, loss = 1.0824034214, val loss = 2.5604419708\n",
      "epoch: 527/2000, loss = 1.1559193134, val loss = 2.5443673134\n",
      "epoch: 528/2000, loss = 1.1668570042, val loss = 2.5290527344\n",
      "epoch: 529/2000, loss = 1.0718884468, val loss = 2.5136098862\n",
      "epoch: 530/2000, loss = 1.0377323627, val loss = 2.4989399910\n",
      "epoch: 531/2000, loss = 1.0309112072, val loss = 2.4843692780\n",
      "epoch: 532/2000, loss = 0.9436869621, val loss = 2.4701642990\n",
      "epoch: 533/2000, loss = 1.0333850384, val loss = 2.4557745457\n",
      "epoch: 534/2000, loss = 1.0644572973, val loss = 2.4417374134\n",
      "epoch: 535/2000, loss = 0.9526106715, val loss = 2.4282469749\n",
      "epoch: 536/2000, loss = 0.9214212298, val loss = 2.4149391651\n",
      "epoch: 537/2000, loss = 1.1604995728, val loss = 2.4010350704\n",
      "epoch: 538/2000, loss = 1.1868767738, val loss = 2.3871412277\n",
      "epoch: 539/2000, loss = 1.0736750364, val loss = 2.3744068146\n",
      "epoch: 540/2000, loss = 1.1619956493, val loss = 2.3616681099\n",
      "epoch: 541/2000, loss = 1.0833574533, val loss = 2.3489687443\n",
      "epoch: 542/2000, loss = 1.0795779228, val loss = 2.3367989063\n",
      "epoch: 543/2000, loss = 1.1305718422, val loss = 2.3245263100\n",
      "epoch: 544/2000, loss = 1.0009700060, val loss = 2.3132040501\n",
      "epoch: 545/2000, loss = 1.0207581520, val loss = 2.3020095825\n",
      "epoch: 546/2000, loss = 1.1263744831, val loss = 2.2909471989\n",
      "epoch: 547/2000, loss = 1.2196382284, val loss = 2.2794392109\n",
      "epoch: 548/2000, loss = 1.0738854408, val loss = 2.2687799931\n",
      "epoch: 549/2000, loss = 1.1560661793, val loss = 2.2581272125\n",
      "epoch: 550/2000, loss = 0.9939460754, val loss = 2.2481701374\n",
      "epoch: 551/2000, loss = 1.0468965769, val loss = 2.2385327816\n",
      "epoch: 552/2000, loss = 1.1477668285, val loss = 2.2284457684\n",
      "epoch: 553/2000, loss = 0.9439243078, val loss = 2.2192871571\n",
      "epoch: 554/2000, loss = 0.9522085190, val loss = 2.2104063034\n",
      "epoch: 555/2000, loss = 1.1561684608, val loss = 2.2010374069\n",
      "epoch: 556/2000, loss = 1.0593943596, val loss = 2.1924514771\n",
      "epoch: 557/2000, loss = 1.0972919464, val loss = 2.1841082573\n",
      "epoch: 558/2000, loss = 1.1266363859, val loss = 2.1754796505\n",
      "epoch: 559/2000, loss = 1.1531608105, val loss = 2.1676321030\n",
      "epoch: 560/2000, loss = 0.9780741930, val loss = 2.1604077816\n",
      "epoch: 561/2000, loss = 0.9457173347, val loss = 2.1534445286\n",
      "epoch: 562/2000, loss = 1.1709424257, val loss = 2.1464574337\n",
      "epoch: 563/2000, loss = 1.1338436604, val loss = 2.1395847797\n",
      "epoch: 564/2000, loss = 1.0728807449, val loss = 2.1331691742\n",
      "epoch: 565/2000, loss = 1.1482584476, val loss = 2.1271779537\n",
      "epoch: 566/2000, loss = 1.1088807583, val loss = 2.1215000153\n",
      "epoch: 567/2000, loss = 1.1496675014, val loss = 2.1160011292\n",
      "epoch: 568/2000, loss = 1.1539000273, val loss = 2.1108751297\n",
      "epoch: 569/2000, loss = 0.9494662881, val loss = 2.1063780785\n",
      "epoch: 570/2000, loss = 1.1764882803, val loss = 2.1019713879\n",
      "epoch: 571/2000, loss = 1.0603345633, val loss = 2.0980193615\n",
      "epoch: 572/2000, loss = 1.0371329784, val loss = 2.0944736004\n",
      "epoch: 573/2000, loss = 1.0823323727, val loss = 2.0911641121\n",
      "epoch: 574/2000, loss = 1.1441388130, val loss = 2.0880937576\n",
      "epoch: 575/2000, loss = 1.1485400200, val loss = 2.0854015350\n",
      "epoch: 576/2000, loss = 1.1067839861, val loss = 2.0830104351\n",
      "epoch: 577/2000, loss = 1.1440451145, val loss = 2.0810174942\n",
      "epoch: 578/2000, loss = 1.1485989094, val loss = 2.0794098377\n",
      "epoch: 579/2000, loss = 1.0600358248, val loss = 2.0782589912\n",
      "epoch: 580/2000, loss = 0.9382284880, val loss = 2.0776121616\n",
      "epoch: 581/2000, loss = 1.1784144640, val loss = 2.0770688057\n",
      "epoch: 582/2000, loss = 1.2113541365, val loss = 2.0769572258\n",
      "epoch: 583/2000, loss = 1.1140245199, val loss = 2.0773146152\n",
      "epoch: 584/2000, loss = 0.9477005601, val loss = 2.0780851841\n",
      "epoch: 585/2000, loss = 1.0739362240, val loss = 2.0791187286\n",
      "epoch: 586/2000, loss = 1.0593755245, val loss = 2.0805356503\n",
      "epoch: 587/2000, loss = 1.1472113132, val loss = 2.0822792053\n",
      "epoch: 588/2000, loss = 1.0354866982, val loss = 2.0845155716\n",
      "epoch: 589/2000, loss = 1.1295611858, val loss = 2.0871455669\n",
      "epoch: 590/2000, loss = 1.0452972651, val loss = 2.0901932716\n",
      "epoch: 591/2000, loss = 1.0493390560, val loss = 2.0935895443\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "t_loss = []\n",
    "\n",
    "vloss = []\n",
    "stop_cr = 0\n",
    "last_best = 1000\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        y_pred = model(inputs)\n",
    "        loss = criteria(y_pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        t_loss.append(loss.item())\n",
    "        \n",
    "            \n",
    "    losses.append(np.mean(t_loss))\n",
    "    t_loss = []\n",
    "            \n",
    "    vloss += [criteria(model(dataset.x_test), dataset.y_test).item()]\n",
    "    if vloss[-1] < last_best:\n",
    "        last_best = vloss[-1]\n",
    "        stop_cr = 0\n",
    "    else:\n",
    "        stop_cr += 1\n",
    "    \n",
    "    if stop_cr == 10:\n",
    "        break\n",
    "        \n",
    "    print(f'epoch: {epoch+1}/{nepochs}, loss = {loss.item():.10f}, val loss = {vloss[-1]:.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2778)"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.softmax(model(dataset.x_test), dim=1).argmax(dim=1).eq(dataset.y_test).sum() / dataset.x_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.plot(vloss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9784455f3e9463d14dc3263833cb7a66fe8439c0a7d698fd0d368a30d65f5d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
